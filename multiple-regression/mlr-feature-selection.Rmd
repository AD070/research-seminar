---
title: "Множественная регрессия: отбор факторов и диагностика модели"
author: "Заходякин Г.В., postlogist@gmail.com"
date: "21.02.2017"
output: 
  github_document: 
    toc: yes
  html_document: 
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
options(width = 100) # ширина текстового вывода
options(digits = 3) # число знаков после запятой в выводе 
```


# Введение

В модель множественной регрессии можно включать несколько объясняющих переменных. Вопрос в том, какие именно переменные лучше всего включить. В этом блокноте мы рассмотрим на примере прогнозирования топливной эффективности машин несколько подходов к отбору факторов в модель. Также здесь рассмотрены вопросы определения наиболее важных предикторов среди включенных в модель и преобразования данных для облегчения интерпретации коэффициентов модели. В заключение будет затронута проблема диагностики модели.

# Подготовка

## Загрузка пакетов

```{r Загрузка пакетов, message=FALSE}
library(readr) # считывание данных из текстовых файлов
library(memisc) # удобное сравнение моделей в таблице
library(tidyverse) # манипулирование данными 
library(ggplot2) # визуализация 
library(ggfortify) # визуализация диагностических графиков
library(modelr) # вспомогательные функции для работы с моделями
library(broom) # преобразование результатов моделирования в табличный вид
library(GGally) # построение матрицы диаграмм рассеяния 
library(car) # функции для степенных преобразований
library(forcats) # работа с факторами

```


## Загрузка данных

```{r Загрузка данных, warning = FALSE, message = FALSE}
cars <- read_csv2("datasets/cars.csv", skip = 25) 
```

Разведочный анализ выполнялся в [первой части примера](mlr-modeling.Rmd).
Здесь мы лишь исключим 5 наблюдений с пропущенными данными.

```{r Удаление строк с пропусками}
cars <- na.omit(cars)
```


# Проблема мультиколлинеарности

## Причина мультиколлинеарности
Рассмотрим матрицу диаграмм рассеяния для различных характеристик машин.

```{r Матрица диаграмм рассеяния, fig.height=10, fig.width=10}

# Матрица диаграмм рассеяния
cars %>% 
  select_if(is.numeric) %>%
  ggpairs(lower = list(continuous = wrap("smooth_lm", color = 'blue')))
```

Все количественные переменные коррелированы с `mpg`. Но можно видеть, что многие из них коррелированы и между собой.
Например, коэффициент корреляции для длины машины и ее колесной базы равен 0.84. Эти переменные передают сходную информацию о машине:

![Колесная база](figures/wheelbase.png)


Термин **"мультиколлинеарность"** (*multicollinearity*) обозначает ситуацию, в которой несколько объясняющих переменных, включенных в модель, коррелированы между собой, т.е. несут дублирующуюся информацию. Мультиколлинеарность - это сильная линейная связь между двумя и более переменными в модели.


## Как проявляется мультиколлинеарность

Рассмотрим на примере, как мультиколлинеарность влияет на результаты моделирования. Для этого добавим к построенной ранее модели с весом и мищностью машины еще одну переменную - `fuel_cap` (емкость топливного бака). Сама по себе эта переменная - хороший предиктор для `mpg` и имеет второй по абсолютной величине коэффициент корреляции с `mpg`: $r = -0.80$. Однако эта переменная сильно коррелирована также и с весом машины: $r = 0.86$.

Изменения в модели удобно отразить в таблице, полученной с помощью функции `memisc::mtable()`.

```{r Изменения в модели при добавлении коррелированного с другими предиктора}
m_wgtpow <- lm(mpg ~ curb_wgt + horsepow, data = cars)
m_wgtpowfuel <- lm(mpg ~ curb_wgt + horsepow + fuel_cap, data = cars)

memisc::mtable(m_wgtpow, m_wgtpowfuel)
```

После добавления еще одной переменной качество модели практически не изменилось: на 0.1 снизилась стандартная ошибка, а коэффициент детерминации остался прежним (с точностью до одного десятичного знака).

Но обратим внимание на то, что стандартная ошибка для коэффициента `curb_wgt` увеличилась почти вдвое. Это означает, что погрешность оценки этого коэффициента увеличилась. Это - одно из негативных проявлений мультиколлинеарности.

Помимо увеличения стандартных ошибок для угловых коэффициентов, мультиколлинеарность затрудняет интерпретацию отдельных коэффициентов. Ранее мы предполагали, что вклад каждого предиктора независим от других и интерпретировали угловой коэффициент $b_j$ как предельный эффект для предиктора $x_j$, т.е. как изменение $y$ при единичном изменении $x_j$ и зафиксированных значениях остальных предикторов. Но поскольку теперь в модели есть взаимосвязанные предикторы, например вес машины и размер топливного бака, больше невозможно изменять эти величины поотдельности. Для более тяжелой машины нужен больший топливный бак, потому что ей нужно больше топлива.

В теории множественной регрессии доказывается, что оценки коэффициентов модели взаимосвязаны между собой. Связь между ними характеризуется **ковариационной матрицей** модели  (*variance-covariance matrix*), которая получается как один из побочных результатов применения метода наименьших квадратов. Эта матрица содержит на главной диагонали дисперсии оценок коэффициентов (т.е. квадраты их стандартных ошибок), а для недиагональных элементов - ковариации оценок соответствующих коэффициентов модели.

```{r Ковариационная матрица}
round(vcov(m_wgtpowfuel), 3)
```

Если два коэффициента имеют большую ковариацию, то стандартная ошибка для коэффициентов увеличится. Также, если один из предикторов исключить из модели, то изменится значение коэффициента для оставшегося предиктора. В некоторых случаях может поменяться даже направленность связи (знак коэффициента). Например, может оказаться, что в модели предсказания спроса цена будет иметь положительный угловой коэффициент, что невозможно с точки зрения здравого смысла.


## Как измерить степень проявления мультиколлинеарности

Для измерения степени выраженности мультиколлинеарности, используется показатель **коэффициента роста дисперсии** (*variance inflation factor*), который вычисляется для каждого предиктора в модели:

$$ VIF_j = \frac{1}{1-R^2_j}, j=1 \ldots k $$

В этой формуле $R^2_j$ - коэффициент детерминации регрессии предиктора $j$ на остальные $k-1$ предикторов. Этот коэффициент показывает, насколько остальные предикторы  повторяют информацию, содержащуюся в предикторе $j$. Для модели с $k=2$ предикторами, $R^2_j$ - это квадрат коэффициента корреляции между этими предикторами.

Если предиктор $j$ не дублирует информацию из других переменных, т.е. не связан с ними линейными зависимостями, то  $R^2_j=0$ и $VIF_j=1$. Если такие зависимости есть, то $VIF_j > 1$. 

Близкое к 1 значение $VIF_j$ позволяет заключить, что мультиколлинеарность при добавлении предиктора $j$ в модель отсуствует или незначительна. Этот предиктор не меняет стандартные ошибки и значения других коэффициентов в модели.

Если значение $VIF_j \gg 1$, то оценка углового для этого коэффициента нестабильна. При изменении набора предикторов коэффициент может сильно измениться, или стать незначимым. Большое значение $VIF$ означает дублирование информации в различных предикторах. Информация, добавленная с переменной, у которой большой $VIF$, уже была объяснена другими переменными в модели.

При $VIF > 4$ можно предположить выраженную мультиколлинеарность.

Для вычисления коэффициентов роста дисперсии в R используется функция `vif()`:

```{r Вычисление коэффициентов роста дисперсии}
vif(m_wgtpowfuel) %>% 
  round(1)
```

Хотя мультиколлинеарность и создает проблемы для статистического вывода (оценки значимости коэффициентов) и интерпретации угловых коэффициентов, она никак не мешает прогнозированию по данной модели. Проблема при прогнозировании возникает только если коэффициентов в модели очень много и для оценки каждого из них приходится всего несколько наблюдений (т.е. отношение числа наблюдений к числу коэффициентов мало). В этом случае модель может переобучиться - запомнить случайный шум, присутствующий в исходных данных вместо закономерных связей. Переобученная модель дает плохой результат при прогнозировании на новых данных.


## Линейная зависимость предикторов

В случае, когда мультиколлинеарность является строгой, т.е. несколько объясняющих переменных в модели связаны совершенной линейной зависимостью, коэффициенты модели невозможно оценить. В формуле для вычисления коэффициентов используется операция обращения матрицы данных, которая может быть выполнена только при условии линейной независимости столбцов.

Вычислим ширину машины в сантиметрах на основе ширины в дюймах и попытаемся оценить модель, в которую включены эти два предиктора. Очевидно, что один из них может быть вычислен на основе другого с помощью умножения на константу, т.е. они линейно зависимы.

```{r Попытка оценить модель с линейно зависимыми предикторами}

cars_multi <- cars %>%
  mutate(width_cm = width * 2.54)

m_width_cm <- lm(mpg ~ width_cm, data = cars_multi)
coef(m_width_cm) # нет проблем

m_width_both <- lm(mpg ~ width + width_cm, data = cars_multi)
coef(m_width_both) # одна из линейно зависимых переменных автоматически исключена

```

## Линейная зависимость между фиктивными переменными

Частным случаем линейной зависимости между предикторами является линейная зависимость между фиктивными переменными (**dummy trap**).
Линейная зависимость между фиктивными переменными возникает, если включить в модель фиктивную переменную для каждой группы. Например, ранее мы строили модель зависимости пробега от мощности с учетом типа машины. Чтобы учесть тип, который является дискретной переменной (грузовик или легковой автомобиль), необходимо преобразовать тип в фиктивные переменные. Мы создали переменную `truck`, котороая принимает значение 1 для грузовиков и 0 для автомобилей. Если же включить в модель  еще одну переменную - `automobile`, которая принимает значение 1 уже для легковых машин, то переменные окажутся линейно зависимыми. Машина может быть либо грузовиком, либо легковым автомобилем, поэтому справедливо равенство:

$$automobile + truck = 1$$

Из этого следует, что зная одну из переменных можно всегда определить значение второй, т.е. они линейно зависимы.


```{r Линейная зависимость для фиктивных переменных}
cars_dummy <- cars %>%
  mutate(truck = ifelse(type == 'Truck', 1, 0), 
         automobile = ifelse(type == 'Automobile', 1, 0))

m_dummy <- lm(mpg ~ horsepow + truck + automobile, data = cars_dummy)
coef(m_dummy) # Включена только одна фиктивная переменная
```

Количество фиктивных переменных, необходимых для учета в модели категориальной переменной, должно быть на единицу меньше числа категорий, чтобы предотвратить линейную зависимость. Вспомним, что базовой категорией будет та, для которой в модель не включена фиктивная переменная. Коэффициенты для включенных в модель фиктивных переменных показывают отличия соответствующих групп от базовой.

В R для моделирования дискретных переменных лучше использовать факторы, при этом фиктивные переменные создаются автоматически и линейной зависимости между ними не может возникнуть в принципе.


# Отбор предикторов в модель


## Полная модель

Попробуем построить **полную модель** (*full model*), включающую всю доступную информацию, т.е. все количественные переменные в наборе данных, а также тип машины и страну производителя.

```{r Полная модель}

m_full <- lm(mpg ~ ., data = dplyr::select(cars, country:mpg)) # Символ . обозначает все переменные в таблице

summary(m_full)

vif(m_full) %>% round(1)

```

У предикторов большие коэффициенты роста дисперсии (VIF). Некоторые угловые коэффициенты незначимы из-за больших стандартных ошибок.
Причина в том, что многие объясняющие переменные взаимосвязаны и несут дублирующуюся информацию. Необходимо упростить модель.


## Критерии для выбора лучшей модели

До сих пор, мы ориентировались на коэффициент детерминации $R^2$ для сравнения моделей. Однако у этого показателя есть ограничения. Можно показать, что при усложнении модели путем включения дополнительных переменных величина $R^2$ всегда увеличивается, даже если новые переменные не несут дополнительной информации. Таким образом, с помощью $R^2$ можно сравнивать только модели, у которых одинаковое число предикторов.

На практике для выбора моделей используются другие показатели, которые на имеют этой проблемы:

 - Скорректированный $R^2$ (Adjusted $R^2$)
 - Информационный критерий Акаике (Akaike Information Criterion, AIC)
 - Модифицированный информационный критерий Акаике, (Corrected AIC, AICc)
 - Байесовский информационный критерий Шварца (Bayesian Information Criterion, BIC)
 - Статистика [Mallow's Cp](https://en.wikipedia.org/wiki/Mallows's_Cp)
 
Все эти показатели используют один и тот же принцип - более сложные модели "штрафуют" за увеличение количества предикторов. К ошибке модели добавляется штраф, который тем больше, чем больше предикторов включено в модель. Рассмотрим это на примере скорректированного $R^2$ и информационных критериев.
 
 
 **Скорректированный  $R^2$** вычисляется по формуле:
 
 $$ R^2_{adj} = 1 - (1-R^2) \cdot \frac{n-1}{n-k-1}$$
 
Чем больше число предикторов $k$, тем больше дробь, и тем больше произведение этой дроби на долю необъясненной дисперсии $(1-R^2)$. Таким образом, доля объясненной дисперсии занижается.


**Информационный критерий Акаике (AIC)** вычисляется на основе дисперсии остатков модели (остаточной суммы квадратов), но он также содержит штрафной компонент, зависящий от числа предикторов: 

$$ AIC = n \ln (RSS/n) + 2(k+2) $$

Здесь штраф включен во второе слагаемое: $k + 2$ - это общее количество оцениваемых по выборке параметров: $k$ угловых коэффициентов, свободный член и стандартная ошибка модели. У AIC нет верхнего и нижнего предела. Меньшее значение AIC соответствует лучшей модели.

Если число наблюдений $n$ невелико, то при сравнении моделей по AIC получаются слишком сложные модели с большим числом предикторов. Поэтому была разработана модификация - **скорректированный AIC (Corrected AIC, AICc)**, в который дополнительно включен штраф за сложность, который линейно уменьшается при увеличении размера выборки:

$$ AICc = AIC + \frac{2 (k+2) (k+3)}{n - k - 3} $$


**Байесовский информационный критерий Шварца (BIC)** вычисляется по аналогичному принципу, однако в нем используется другой штраф:

$$ BIC = n \ln (RSS/n) + (k+2) \cdot \ln n $$

В BIC штраф за увеличении сложности выше, поэтому при использовании этого критерия отбора предпочтение отдается моделям с меньшим числом предикторов.

```{r Сравнение штрафов в AIC и BIC, echo=FALSE}

f_AIC <- function(k) { 
  2 * (k + 2)
}

f_AICc <- function(k, n) {
  f_AIC(k) + 2 * (k + 2) * (k + 3) / (n - k - 3)
}

f_BIC <- function(k, n) {
  (k + 2) * log(n)
}


ggplot(data = tibble(k = seq(0, 30)), 
       mapping = aes(x = k)) +
  stat_function(mapping = aes(colour = "AIC"), 
                fun = f_AIC) +
  
  stat_function(mapping = aes(colour = "AICc, n = 50"), 
                fun = f_AICc, args = list(n = 50)) +
  
  stat_function(mapping = aes(colour = "AICc, n = 500"), 
                fun = f_AICc, args = list(n = 500), 
                linetype = 'dashed' ) +


  stat_function(mapping = aes(colour = "BIC, n = 50"), 
                fun = f_BIC, args = list(n = 50)) +
  
  stat_function(mapping = aes(colour = "BIC, n = 500"), 
                fun = f_BIC, args = list(n = 500), 
                linetype = 'dashed' ) +
  
  scale_colour_manual(values = c("gold", "red", "darkred", "lightskyblue", "blue")) +
  
  labs(title = "Сравнение штрафов AIC, AICc и BIC",
       x = "Число параметров k",
       y = "Штраф")

```



## Метод полного перебора

Для небольших наборов данных можно применить для выбора наилучшей модели (по заданному критерию) метод полного перебора. Однако число моделей, которые необходимо рассмотреть экспоненциально возрастает, поэтому данный подход невозможно масштабировать.

В R есть функция `leaps::regsubsets()`, которая решает задачу выбора наилучшей модели при заданном числе предикторов (*best subsets regression*). Эта функция использует сравнивает модели на основе скорректированного $R^2$.

```{r Выбор лучшей модели полным перебором}
best_models <- leaps::regsubsets(mpg ~ ., # рассматриваем все возможные предикторы
                                 nbest = 2,  # две лучших модели для каждого числа предикторов
                                 nvmax = 11, # максимально 11 переменных в модели, по умолчанию - 8
                                 data = dplyr::select(cars, country:mpg))

# Сводка по моделям
summary(best_models)

```


Можно визуализировать результаты сравнения моделей средствами базовой графики R.

```{r Визуализация результатов сравнения моделей, fig.height = 7}

# Создаем палитру менее депрессивных цветов
# См. RColorBrewer::display.brewer.all()
mycolors <- rev(RColorBrewer::brewer.pal(n = 5, name = 'Greens')) 

# Визуализация моделей
plot(best_models, scale = "adjr2", col = mycolors,
     main = "Сравнение лучших моделей по скорректированному R^2")

```

На этом графике модели упорядочены по убыванию скорректированного $R^2$. Каждой модели соответствует одна строка в матрице. Ячейки, закрашенные цветом - это выбранные предикторы, белые ячейки - не включенные в модель переменные. 

Вверху находится наилучшая по этому критерию модель, в которую включены все предикторы, кроме длины и ширины машины.

Если выбрать другой критерий ранжирования, то лучшей окажется другая модель. Например, при выборе BIC, в котором используется больший штраф за усложнение модели, результат получится другой:

```{r Лучшая модель по BIC, fig.height = 7}
# Визуализация моделей
plot(best_models, scale = "bic", col = mycolors, 
     main = "Сравнение лучших моделей по BIC")

```

Здесь в лучшей модели (с наименьшим BIC) оказалось гораздо меньше переменных - 5 вместо 8.


** Ограничения метода **
Помимо неоднозначности результатов в зависимости от выбора критерия оценки моделей, и больших вычислительных затрат, у метода полного перебора есть еще один недостаток: если зависимость между переменными нелинейная и нужно преобразование данных, то такое преобразование автоматически выполняться не будет. Ответственность за выяснение этой необходимости и подготовка данных лежит на аналитике. Также не рассматриваются взаимодействия предикторов.


## Пошаговая регрессия

**Пошаговая регрессия** (*stepwise regression*) - это эвристический алгоритм для отбора переменных в регрессионную модель, который можно применять даже когда количество возможных предикторов велико и полный перебор невозможен. Алгоритм работает итеративно. На каждом шаге принимается решение, какую переменную лучше всего включить в модель, или исключить из нее, чтобы повысить точность. Существуют три модификации алгоритма пошаговой регрессии:

- Пошаговое исключение переменных (*backward stepwise*) - путь упрощения сложной модели
- Пошаговое включение переменных (*forward stepwise*) - путь усложнения простой модели
- Пошаговый перебор (*Stepwise stepwise*) - путь последовательного изменения модели, при этом на каждом шаге она может быть усложнена или упрощена


Метод пошагового исключения начинает с полной модели и последовательно исключает наихудший предиктор на каждом шаге, до тех пор пока не окажется, что дальнейшее упрощение модели ухудшает ее качество. 

Метод пошагового включения действует в обратном напралении: работа начинается с нулевой модели без предикторов, и на каждом шаге определяется, какую из переменных лучше всего включить в модель как предиктор.

Метод пошагового отбора после включения на очередном шаге новой переменной в модель, может исключать из модели добавленные ранее переменные, если это улучшит точность. Такая необходимость возникает в том случае, если переменные коррелированы и добавленная на более позднем шаге переменная содержит ту же информацию, что и ранее добавленная.

Для оценки качества модели на каждом шаге используется какой-либо "штрафной" критерий - например, AIC или скорректированный $R^2$.

В R алгоритм пошаговой регрессии реализован функцией `MASS:stepAIC()`. 

Рассмотрим использование алгоритма пошаговой регрессии на примерах.


Метод пошагового исключения основан на упрощении первоначальной модели. Чтобы задать эту модель, можно использовать формулу, или ранее полученную с помощью `lm()` модель. 

```{r Backward Stepwise}

m_backward <- stepAIC(m_full, direction = "backward")

```

При необходимости, можно отключить вывод результатов на каждом шаге, задав параметр `trace = F`. Можно менять весовой коэффициент для штрафа (параметр `n`). По умолчанию он равен 2 (AIC), если задать величину log(N), где `N` - число наблюдений в выборке, то получится BIC.


Для использования пошагового включения или перебора необходимо задать направление поиска, указав наиболее простую и наиболее полную модели, которые надо рассмотреть, в параметре `scope = `. Здесь мы используем в качестве границ нулевую и полную модели.

Пошаговое включение

```{r Forward Stepwise}

# Нулевая модель
m_null <- lm(mpg ~ 1, data = dplyr::select(cars, country:mpg))

# Прямое включение
m_forward <- stepAIC(m_null, 
                     scope = list(lower = m_null, upper = m_full),
                     direction = 'forward')

```


Пошаговый перебор

```{r Stepwise Stepwise}
m_stepwise <- stepAIC(m_null, 
                      scope = list(lower = m_null, upper = m_full), 
                      direction = 'both')
```


Сравним модели, полученные пошаговым отбором, и лучшую модель, полученную полным перебором вариантов.


```{r Сравнение моделей при пошаговом отборе}

m_best <- lm(mpg ~ type + horsepow + curb_wgt + fuel_cap, data = cars)

memisc::mtable(m_backward, m_forward, m_stepwise, m_best)
```


** Ограничения метода **

В данном случае, все три алгоритма последовательного перебора привели к одинаковому результату. Он, тем не менее, отличается от лучшей модели, полученной полным перебором. У этой модели меньше предикторов, и значение BIC лучше, чем у моделей, полученных последовательным перебором. Возможно, разница результатов объясняется тем, для отбора моделей использовались разные критерии (AIC и BIC), но возможна и другая причина. Алгоритмы пошагового отбора - "жадные", они принимают локально оптимальное решение на каждом шаге и не могут вернуться назад, если поиск зайдет в тупик. Поэтому гарантий нахождения глобального оптимума - наилучшей возможной модели - нет. 

Кроме того, как и в методе полного перебора, ответственность за нелинейные преобразования данных или учет взаимодействия предикторов лежит на аналитике. Методы их автоматически не рассматривают.



