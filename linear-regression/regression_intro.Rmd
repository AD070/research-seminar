--- 
title: "Модель линейной регрессии" 
author: "Заходякин Г.В., postlogist@gmail.com" 
date: "06.02.2017" 
output: html_document 
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE, warning = FALSE) 
```

# Введение

## Задача регрессии

**Задача регрессии** состоит в предсказании **непрерывной выходной переменной**
(*response, outcome, dependent variable*) на основе одной или нескольких
**объясняющих переменных** (*independent, explanatory, predictor variables*).

![Задача регрессии](figures/regression.png)

Для решения задачи регрессии применяются различные классы моделей. Наиболее
простым видом являются модели **линейной регрессии** (простой и множественной).
Несмотря на простую структуру, эти модели можно успешно применять во многих
ситуациях.

Метод линейной регрессии позволяет:

- выявить, какие факторы влияют на выходную переменную, - количественно описать
зависимость между объясняющими переменными и выходной переменной, -
прогнозировать значения выходной переменной, - обнаруживать в данных нетипичные
наблюдения, которые могут быть результатом ошибок или мошеннических действий.

В этом примере рассматривается метод простой линейной регрессии и построение
регрессионной модели на конкретном примере. Затронуты вопросы оценки значимости
регрессионной модели и статистического вывода на ее основе.

## Описание примера

Сыр Чеддер (Cheddar) - это довольно твердый бесцветный или оранжевый (при
добавлении специй и красителя Аннато) сорт сыра со слегка острым и кисловатым
вкусом.  Своим названием Чеддер обязан небольшому поселку в графстве Соммерсет
(Англия).  Сегодня Чеддер производится не только в "родном" регионе, но и во
многих странах мира. Это самый популярный сорт сыра в Великобритании, занимающий
51% на рынке. В целом, оборот на сырном рынке в Великобритании составляет £1.9
млрд.

![Чеддер](figures/Somerset-Cheddar.jpg)

Особенно острые виды Чеддера длительное время (до нескольких лет) выдерживаются
в специальных условиях. Одним из требований является постоянство температуры,
которое обеспечивается либо с помощью холодильного оборудования, либо благодаря
использованию природных "термостатов" - пещер. В пещерах создаются идеальные
условия для вызревания сыра. До сих пор в некоторых регионах Чеддер выдерживают
в пещерах (Wookey Hole, Cheddar Gorge).

![Cheddar cave](figures/Cheddar_cave_cheese.jpg)

В процессе созревания чеддера протекают различные биохимические процессы. Вкус
сыра связан с концентрацией нескольких веществ в конечном продукте. Мы
располагаем данными лабораторных исследований чеддера (LaTrobe Valley,
Австралия, штат Виктория), в которых изучался химический состав образцов сыра, а
затем проводилась экспертная оценка вкусовых качеств. В наборе данных содержится
усредненная оценка вкуса различных образцов сыра несколькими экспертами и
химический состав каждого образца. Набор данных загружен из репозитория
[DASL](http://dasl.datadesk.com/story/view/21).

**Целями анализа** являются:

- Изучение зависимости между составом сыра и оценкой вкусовых качеств. - Выбор
наилучшей однофакторной модели для предсказания оценки вкусовых качеств. -
Прогнозирование оценки вкусовых качеств сыра на основе его состава.


# Анализ 
## Подготовка

Функция для построения линейных моделей входит в базовый пакет `stats`, однако
для загрузки, визуализации данных и облегчения работы с моделью необходимо
использовать дополнительные пакеты.

```{r Загрузка пакетов} 
suppressMessages(library(readr)) # считывание данных из текстовых файлов
suppressMessages(library(tidyverse)) # манипулирование данными 
suppressMessages(library(ggplot2)) # визуализация 
suppressMessages(library(GGally)) # построение матрицы диаграмм рассеяния 
suppressMessages(library(ggfortify)) # визуализация диагностических графиков
suppressMessages(library(modelr)) # вспомогательные функции для работы с моделями 
```

Данные содержатся в файле `datasets/cheese.txt`. Первые 16 строк файла -
комментарий.

```{r Загрузка данных} 
cheese <- read_tsv('data/cheese.txt', skip = 16)

glimpse(cheese) # Структура данных 
summary(cheese) # Описательная статистика + проверка пропущенных значений 
```

Типы данных распознаны правильно. Пропущенных значений нет.

Переменные:

- **Case** - номер образца - **Taste** -  экспертная оценка вкуса (средний балл,
выставленный экспертами) - **Acetic** - содержание уксусной кислоты (натуральный
логарифм) - **H2S** - содержание сероводорода (натруальный логарифм) -
**Lactic** - содержание молочной кислоты (натруальный логарифм)

## Исследование зависимостей в данных

Когда переменных немного, зависимости между каждым фактором и целевой переменной
можно искать с помощью обычных диаграмм рассеяния. Для быстрой оценки удобно
использовать **матрицу диаграмм рассеяния** (*scatterplot matrix, SPLOM*). В
базовой графике эту визуализацию можно получить с помощью функций `pairs()` или
`car::scatterplotMatrix()`.

Для визуализации с помощью `ggplot2` можно воспользоваться функцией
`GGally:ggpairs()`. В отличие от стандартной функции, `ggpairs()` позволяет
работать и с дискретными переменными.

Узнать о том, как настроить вид графика под свои нужды, можно в
[справке](http://ggobi.github.io/ggally/#ggallyggpairs).

```{r Матрица диаграмм рассеяния} ggpairs(cheese, columns = c("Acetic", "H2S",
"Lactic", "taste"), # исключили номер образца lower = list( continuous =
wrap("smooth_lm", color = 'blue') # добавили тренд и покрасили точки ) ) ```

На главной диагонали содержатся плотности распределения соответствующих
переменных, над ней - коэффициенты корреляции Пирсона между парами переменных.
Под диагональю содержатся диаграммы рассеяния для каждой пары переменных.
Степень влияния содержания различных веществ на вкус можно визуально оценить на
основе нижней строчки матрицы.

Когда переменных много и надо показать только связь факторов и целевой
переменной, можно воспользоваться стандартными возможностями `ggplot()`, однако
придется изменить структуру данных. Данные надо перевести в "длинный" формат,
слив все столбцы в один. Для этого можно воспользоваться функцией
`tidyr::gather()` (см.
http://r4ds.had.co.nz/tidy-data.html#spreading-and-gathering)

![Пример работы gather()](http://r4ds.had.co.nz/images/tidy-9.png)


```{r Перевод в длинный формат}

cheese_tall <- cheese %>% gather(key = 'Substance', value = 'Content',
Acetic:Lactic)

```


Теперь можно изучить влияние содержания отдельных веществ на вкус.


```{r Визуализация влияния отдельных веществ}

ggplot(data = cheese_tall, mapping = aes(x = Content, y = taste, color =
Substance)) + geom_point() + geom_smooth(method = 'lm', se = F) + facet_wrap(~
Substance, scales = "free_x") + labs(title = 'Влияние содержания отдельных
веществ на вкус', y = 'Оценка вкуса', x = NULL)

```


## Корреляция: количественная оценка степени линейной связи

**Коэффициент корреляции Пирсона** $r$ (*The Pearson's product-moment
correlation*) позволяет количественно оценить степень выраженности (тесноту)
линейной связи между двумя непрерывными переменными:

$$ r = \frac{cov(x, y)}{S_x S_y} =
\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{n S_x S_y} $$

Для вычисления коэффициента корреляции в R можно использовать встроенную функцию
`cor()`. Функция позволяет вычислять корреляцию между парой числовых переменных
(векторов), или для всех переменных в наборе данных сразу.


```{r Коэффициенты корреляции} 
# Для двух столбцов: 
cor(cheese$taste, cheese$Acetic)

# Для всех столбцов (кроме Case): 
options(digits = 3) # число знаков после запятой в выводе 
corr.matrix <- cor(select(cheese, -Case)) 
corr.matrix

```

## Почему нужна визуализация?

Во встроенном наборе данных `anscombe` содержится 4 примера данных - т.н.
[квартет Энскомба](http://tinyurl.com/anscombe-quartet).

```{r Исходная структура anscombe} 
head(anscombe, 3) 
```

Для удобства, преобразуем данные в "длинный" формат: *номер примера, x, y*.

```{r Преобразование структуры данных anscombe} 
anscombe_tall_x <- anscombe %>%
  select(x1:x4) %>% 
  gather(key = 'case', value = 'x') %>%
  mutate(case = stringr::str_sub(case, start = 2))

anscombe_tall_y <- anscombe %>% 
  select(y1:y4) %>% 
  gather(key = 'case', value = 'y') %>% 
  select(-case)

anscombe_tall <- bind_cols(anscombe_tall_x, anscombe_tall_y)

head(anscombe_tall, 3) 
```

Рассчитаем коэффициенты корреляции:

```{r Коэффициенты корреляции для примера anscombe}

anscombe_tall %>% group_by(case) %>% summarize(r = cor(x, y))

```

Во всех 4 примерах коэффициенты корреляции равны с точностью до двух десятичных
знаков. Однако визуализация данных демонстрирует совершенно различный характер
зависимостей:

```{r Визуализация данных anscombe} 
ggplot(data = anscombe_tall, mapping = aes(x, y)) + 
  geom_point() + 
  facet_wrap(~ case, ncol = 2, scales = 'free_x') + 
  geom_smooth(method = 'lm', se = F, color = 'red')

```

Хотя коэффициент корреляции позволяет количественно оценить выраженность связи,
есть ограничения в его применении:

- выявляются только линейные зависимости, - чувствительность к выбросам в
данным.

## Реальна ли обнаруженная зависимость?

Если образцы для лабораторных исследований отбирались случайным образом, то не
должно существовать никаких зависимостей показателей от номера образца.

```{r Коэффициенты корреляции между номером образца и остальными переменными}

# Количественная оценка выраженности связи 
cor(cheese)['Case', ] 
```

Тем не менее, коэффициенты корреляции для переменной `Case` не нулевые.

```{r Визуализация зависимостей для номера образца} 
# Зависимость оценки вкуса от номера образца 
ggplot(data = cheese, mapping = aes(x = Case, y = taste)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F)

# Зависимость содержания веществ от номера образца 
ggplot(data = cheese_tall, 
       mapping = aes(x = Case, y = Content)) + 
  geom_point() + 
  geom_smooth(method ='lm', se = F) + 
  facet_wrap(~ Substance, scales = 'free_y')

```


### Имитационный эксперимент

На практике из-за случайной вариации данных полученные на выборке коэффициенты
корреляции будут отличными от нуля даже для совершенно независимых переменных.
Покажем это с помощью имитационного эксперимента, в котором создадим 2
равномерно распределенные случайные величины.

```{r Генерация двух случайных переменных} 
set.seed(3) 
independent <- tibble( x = runif(n=30, min=-1, max=1), 
                       y = runif(n=30, min=-1, max=1) )

r_independent <- with(independent, cor(x, y))

ggplot(data = independent, mapping = aes(x, y)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F) + 
  geom_text(x = -1, y = -1, color = 'blue',
            vjust = -1, hjust = -0.1, size = 8, 
            label = paste('r =', round(r_independent,2))) 
```

Коэффициент корреляции Пирсона $r$ - это выборочная статистика, поэтому, как и у
каждой выборочной статистики, у него есть выборочное распределение. Вспомним,
что выборочное распределение - это распределение изучаемой выборочной
статистики, которое может быть получено путем многократного повторения
выборочного исследования при том же самом размере выборки.

На основе свойств выборочного распределения для $r$ можно проверить
статистическую гипотезу о равенстве нулю коэффициента корреляции для
совокупности:

$$ H_0: \rho_{xy} = 0 $$

$$ H_1: \rho_{xy} \ne 0 $$

В R для проверки значимости коэффициента корреляции используется функция
`cor.test()`:

```{r Проверка значимости коэффициента корреляции для случайных данных}

with(independent, cor.test(x, y))
```

Для проверки гипотезы:

$$H_0: \rho = 0$$ против альтернативной: $$H_1: \rho \ne 0$$

используется одновыборочный t-критерий.

Статистика критерия:

$$ t = \frac{r - 0}{SE(r)}$$

В сгененрированной выборке с двумя случайными независимыми переменными
вероятность получить такое же, или большее по модулю, значение коэффициента
корреляции равнна 20%. Таким образом, нет оснований отвергнуть нулевую гипотезу.
Рассчитанное по выборке значение $r$ статистически не значимо. То есть, оно не
говорит о наличии связи между $x$ и $y$ в генеральной совокупности.


### Статистическая значимость связей для вкусовых качеств сыра

В примере с данными о вкусовых качествах сыра, результаты проверки значимости
коэффициентов корреляции следующие:

```{r Проверка значимости коэффициентов корреляции для данных о сыре} 
with(cheese, cor.test(taste, Case)) 
with(cheese, cor.test(taste, Acetic)) 
with(cheese, cor.test(taste, H2S)) 
with(cheese, cor.test(taste, Lactic)) 
```

Все корреляции, кроме корреляции с номером образца `Case` значимы. Таким
образом, можно сделать вывод о наличии линейной связи между содержанием
химических веществ и вкусом сыра. Эту связь можно промоделировать с помощью
линейной регрессии.


Для удобства анализа значимости коэффициентов корреляции для больших наборов
данных, можно использовать функцию `HMisc::rcorr()`, которая возвращает
p-значения вместе с со значениями коэффициента корреляции.

```{r Анализ значимости коэффициентов} 
cheese_corr <- Hmisc::rcorr(as.matrix(cheese)) 
cheese_corr 
```

См. также примеры визуализации корреляционной матрицы с помощью пакета
`ggcorrplot`:
http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2


# Моделирование зависимостей

## Модель линейной регрессии

**Модель линейной регрессии** (*the simple linear regression  model*) позволяет
предсказывать значение зависимой переменной на основании только одной
объясняющей переменной. Между этими переменными предполагается линейная
зависимость в генеральной совокупности:

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

Параметры модели $\beta_0$ и $\beta_1$ определяют **свободный член**
(*intercept*) и **угловой коэффициент** (*slope*)  прямой соответственно.

Свободный член $\beta_0$ определяет точку пересечения прямой с осью ординат,
т.е. ожидаемое значение $y$ при $x=0$. Угловой коэффициент $\beta_1$ определяет
величину изменения $y$ при единичном изменении $x$.

**Остаток** (*error term*) $\epsilon$ учитывает случайные отклонения фактических
наблюдений от прогноза по линейной модели.

Можно считать, что каждое наблюдение  $y_i$ включает два компонента -
систематический, $\beta_0 + \beta_1 x_i$ (объясняется моделью) и случайный,
$\epsilon_i$, - (ошибка модели. К случайному компоненту относится действие всех
прочих факторов, помимо $x$.

Как правило, истинные значения коэффициентов $\beta$ неизвестны, т.к. нельзя
исследовать всю совокупность. В этом случае необходимо оценивать их по выборке.
Уравнение регрессионной прямой для выборки имеет вид:

$$ y_i = b_0 + b_1 x_i + e_i $$

Это уравнение по структуре аналогично регрессионной прямой для совокупности,
отличие лишь в том, что коэффициенты и остаток оцениваются по выборке. Найденные
значения являются **оценками** соответствующих параметров совокупности: $$b_j
=\hat{\beta_j},$$


$$e_i = \hat{\varepsilon_i}.$$

Символ `^` (*hat*) обозначает оценку параметра по данным выборки.

Пример линейной модели мы уже видели на диаграммах рассеяния, построенных с
помощью ggplot:

```{r Регрессионная прямая для вкуса и содержания молочной кислоты} 
ggplot(data = cheese, mapping = aes(Lactic, taste)) + 
  geom_point() + 
  geom_smooth(method=lm, se=F, color='blue') + 
  labs(title = 'Регрессионная прямая для оценки вкуса\nвзависимости от содержания молочной кислоты')

```

Обратим внимание, что наблюдения не лежат на регрессионной прямой, а рассеяны
вокруг нее. Это означает, что модель линейной регрессии не может полностью
объяснить разброс экспертных оценок. Существует ошибка, обусловленная не
учтенными факторами. Разность между фактическим $y_i$ и прогнозным $\hat{y_i}$
значением для каждого наблюдения в выборке называется **остатком** (*residual*):

$$ e_i = y_i - \hat{y_i} = y_i - (b_0 + b_1 x_i) $$



### Связь корреляции и регрессии

Коэффициенты линейной модели оцениваются на основе выборки **методом наименьших
квадратов, МНК** (англ *Ordinary Lest Squares, OLS*). Коэффициенты выбираются
таким образом, чтобы сумма квадратов остатков (**остаточная сумма квадратов**)
была минимальной:

$$ \sum_{i=1} ^N e_i^2  \to min \Rightarrow b_0, b_1 $$

Из этого условия можно получить в явном виде формулы для расчета коэффициентов.
Поскольку коэффициенты всегда оцениваются с помощью компьютерных программ, сами по себе эти формулы не интересны. Однако можно привести их к следующему виду:

$$ b_1 = r \frac{S_y}{S_x} $$

$$ b_0 = \bar{y} - b_1 \bar {x} $$

Первая формула связывает угловой коэффициент $b_1$ и коэффициент корреляции Пирсона  $r$. Угловой коэффициент $b_1$ пропорционален $r$ и имеет тот же знак, т.к. стандартные отклонения $S_x$ и $S_y$ всегда положительны. Таким образом, знак коэффициента корреляции позволяет определить направление регрессионной прямой.

Вторая формула говорит о том, что регрессионная прямая всегда проходит через точку:
$$(\bar{x}, \bar{y})$$

## Линейная регрессия в R

### Оценка модели

Для оценки моделей линейной регрессии в R применяется встроенная функция `lm()` (linear model). Эта функция может работать с векторами данных, но обычно ее применяют к наборам данных. Для спецификации модели используется запись в виде формулы: `y ~ x`, что обозначает модель для `y` в зависимости от `x`. По умолчанию оценивается модель со свободным членом $b_0$. Если его необходимо исключить, то спецификацию модели следует изменить на: `y ~ x -1`.

Построим линейную регрессию для оценки вкуса в зависимости от содержания молочной кислоты:

```{r Модель для оценки в зависимости от содержания молочной кислоты} 
m_Lactic <- lm(taste ~ Lactic, data=cheese) 
# Выделение коэффициентов
coef_Lactic <- coef(m_Lactic) 
coef_Lactic 
```

$$\widehat{taste} = -29.9 + 37.7 Lactic$$

Таким образом, при увеличении содержания молочной кислоты на 1, оценка вкуса, в среднем, увеличивается на 37.7.

Свободный член равен -29.9. Это значение соответствует содержанию молочной кислоты 0. В данном случае свободный член не имеет содержательной интерпретации, т.к. минимальное значение оценки вкуса равно нулю, а содержание молочной кислоты не может быть нулевым на практике.

Функция `lm()` возвращает объект, из которого можно извлекать нужный компонент по имени или при помощи специальных функций.

```{r Структура объекта для lm}

names(m_Lactic) 

```

Наиболее часто используемые функции для работы с линейной моделью приведены в таблице:

Функция             | Возвращаемое значение
--------------------|---------------------------------------------------- 
`coef()`            | Вектор коэффициентов модели 
`confint()`         | Таблица с доверительными интервалами для коэффициентов
`fitted()`          | Вектор с прогнозами по модели на обучающей выборке
`residuals()`       | Вектор с остатками модели 
`anova()`           | Таблица дисперсионного анализа для модели

### Визуализация модели

Наиболее простой способ визуализации модели линейной регрессии - с помощью функции `geom_smooth()` пакета `ggplot`. Однако этот способ позволяет визуализировать только однофакторные модели. В дальнейшем этой возможности будет недостаточно. 

Другим способом являеся визуализация с помощью данных подгонки модели, возвращаемых функцией `lm()`. Можно вручную извлечь эти данные с помощью функции `fitted()` и добавить их в набор данных, либо воспользоваться функцией `modelr::add_predictions()`.

```{r Визуализация модели}
pred_Lactic <- cheese %>% add_predictions(m_Lactic)

ggplot(data = pred_Lactic) +
  geom_point(aes(Lactic, taste)) +
  geom_line(aes(Lactic, pred), color = 'red') +
  labs('Регрессионная прямая для оценки вкуса\nв зависимости от содержания молочной кислоты')

```

В этом примере мы использовали для построения линии те же данные, что и в исходном наборе. Вместо этого можно создать последовательность (сетку) значений предикторов в произвольном диапазоне, получить прогноз и использовать для построения линии новые данные. Воспользуемся функцией `modelr::data_grid()`, которая возвращает набор данных с уникальными сочетаниями всех выбранных переменных исходного набора данных.

```{r Визуализация модели по новым данным}

grid_Lactic <- cheese %>%
  data_grid(Lactic) %>%
  add_predictions(m_Lactic)

ggplot(data = pred_Lactic) +
  geom_point(aes(Lactic, taste)) +
  geom_line(aes(Lactic, pred), data = grid_Lactic, color = 'red') +
  labs('Регрессионная прямая для оценки вкуса\nв зависимости от содержания молочной кислоты')

```


### Реальна ли полученная зависимость

Вернемся к примеру со случайными данными:

```{r Прямая регрессии для случайных данных}
ggplot(data = independent, mapping = aes(x, y)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F) + 
  geom_text(x = -1, y = -1, color = 'blue',
            vjust = -1, hjust = -0.1, size = 8, 
            label = paste('r =', round(r_independent,2))) 
```

Несмотря на то, что переменные независимы, они имеют ненулевую корреляцию. Как следствие, регрессионная прямая для выборки также имеет не нулевой наклон. При использовании функции `lm()` будут получены ненулевые коэффициенты.

```{r Fitting a linear model for two independent variables}

m_ind <- lm(y ~ x, data = independent) 
coef(m_ind) 

```


Как и в случае с $r$, в выборочных исследованиях необходимо проверять значимость найденных по выборке коэффициентов регрессии.  Для этого также используется одновыборочный t-критерий с нулевой гипотезой равенства коэффициента нулу в совокупности:

$$ H_0: \beta_j = 0 $$ 

$$ H_1: \beta_j \ne 0 $$

Проверка автоматически выполняется встроенной функцией `summary()`.

```{r Значимость коэффициентов регрессии для случайных данных}
summary(m_ind)
```

В столбце `Estimate` содержится оценка коэффициента, далее выводится его стандартная ошибка и статистика t-критерия:

$$ t_j = \frac{b_j - 0}{SE(b_j)}$$
Последний столбец содержит p-значение для критерия (используется двухсторонний критерй).
В этом примере оба коэффициента модели незначимы, т.к. $p > 0.05$ и нет оснований отвергнуть нулевую гипотезу.

Другим способом проверки значимости коэффициентов модели является использование функции `confint()`, возвращающей доверительные интервалы для коэффициентов.

```{r Доверительные интервалы для коэффициентов со случайными данными}
confint(m_ind)
```


Оба интервала включают 0, поэтому гипотезу о равенстве нулю соответствующего коэффициента для совокупности нельзя отвергнуть на 5% уровне значимости.

Теперь оценим значимость коэффициентов модели для вкусовых качеств сыра.

```{r Значимость коэффициентов модели для сыра, фактор - молочная кислота}
summary(m_Lactic)
confint(m_Lactic)

```

Оба коэффициента оказались значимы, поскольку соответствующие p-значения меньше порога 5%, а доверительные интервалы для них не включат 0.

# Прогнозирование 

Мы выяснили, что зависимость между содержанием молочной кислоты и оценкой вкуса не случайна, поэтому полученную модель можно использовать для предсказания вкусовых качеств на основе данного предиктора.

Уравнение для прогнозирования может быть получено с помощью найденных коэффициентов:

$$ \widehat{taste} = - 29.9 + 37.7 Lactic $$
Например, оценка вкусовых качеств сыра с содержанием молочной кислоты 1.5 составит:

```{r Применение уравнения регрессии вручную}
-29.9 + 37.7 * 1.5
```

Полученный результат - это ордината одной из точек на регрессионной прямой. 
Однако мы видели, что большинство точек не лежат на регрессионной прямой, поэтому полезно оценить степень неопределенности, связанную с прогнозом. Это можно сделать при помощи **интервального прогноза**. Интервальный прогноз - это область вокруг точечного прогноза, в которой с заданной **доверительной вероятностью** будут содержаться фактические значения (доверительный интервал для прогноза), или условное математическое ожидание для $y$ в точке $x$ для совокупности (доверительный интервал для средних).

В R интервальный прогноз для среднего можно получить непосредственно используя `geom_smooth()`. В числовом виде - с помощью функции `predict()`.

```{r Доверительный интервал для среднего}

ggplot(data = pred_Lactic, mapping = aes(Lactic, taste)) +
  geom_point() +
  geom_smooth(method = 'lm', 
              color = 'red') +
    geom_smooth(method = 'lm',
              level = .8,
              color = 'red') +
  labs('Регрессионная прямая для оценки вкуса\nв зависимости от содержания молочной кислоты')

```


```{r Интервальный прогноз для отдельных наблюдений}

# Сетка данных
grid_Lactic2 <- cheese %>%  data_grid(Lactic)

# Добавляем прогноз
pi_Lactic <- predict(m_Lactic, 
                        newdata = grid_Lactic2,
                        interval = 'prediction',
                        level = 0.95)

ci_Lactic <- predict(m_Lactic, 
                        newdata = grid_Lactic2,
                        interval = 'confidence',
                        level = 0.95)


pi_Lactic2 <- bind_cols(grid_Lactic2, as.data.frame(pi_Lactic))
ci_Lactic2 <- bind_cols(grid_Lactic2, as.data.frame(ci_Lactic))
```

```{r Визуализация интервального прогноза}

ggplot(data = pred_Lactic) +
  geom_ribbon(aes(Lactic, ymin = lwr, ymax = upr), 
              data = pi_Lactic2,
              fill = 'lightskyblue') + 
  
  geom_ribbon(aes(Lactic, ymin = lwr, ymax = upr), 
              data = ci_Lactic2,
              fill = 'lightgray') +
  
  geom_line(aes(Lactic, fit), data = pi_Lactic2, color = 'red') +

  geom_point(aes(Lactic, taste)) +

  labs(title = 'Интервальный прогноз оценки вкуса\nв зависимости от содержания молочной кислоты')


```


# Задания

1. Постройте две модели простой линейной регрессии для зависимостей оценки вкуса от содержанию двух других веществ. Запишите для каждой модели формулу для прогнозирования оценки (на основе вывода lm).

2. Проверьте значимость коэффициентов модели с помощью t-критериев. Напишите вывод о результатах проверки значимости с указанием нулевой и альтернативной гипотезы, интерпретацией вывода функции R.

3. Постройте доверительные интервалы для среднего значения и отдельных наблюдений.

4. На основе  сравнения доверительных интервалов, сделайте вывод о том, какая модель наиболее точная.
