---
title: "Введение в mlr"
author: "Заходякин Г.В., postlogist@gmail.com"
date: '15 мая 2017 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(mlr)
library(stringr)
library(forcats)
```

```{r}
car_ad <- read_csv("car_ad.csv")
glimpse(car_ad)
```

```{r}
car_ad %>% 
  group_by(car) %>%
  summarize(count = n()) %>%
  arrange(count) %>%
  filter(count >= 100) %>%

  ggplot(aes(x = fct_inorder(car), y = count)) +
    geom_bar(stat = "identity") + coord_flip()

```


```{r}
car_ad %>% 
  group_by(model) %>%
  summarize(count = n()) %>%
  arrange(count) %>%
  filter(count > 100) %>%

  ggplot(aes(x = fct_inorder(model), y = count)) +
    geom_bar(stat = "identity") + coord_flip()
```

```{r}
car_ad %>% 
  group_by(car) %>%
  summarize(n_models = n_distinct(model)) %>%
  arrange(n_models) %>%
    
  filter(n_models >= 20) %>%
  ggplot(aes(x = fct_inorder(car), y = n_models)) +
   geom_bar(stat = "identity") + coord_flip()

```


```{r}
# Преобразование текстовых переменных в факторные: mlr не работает с текстом
car_ad1 <- car_ad %>% 
  mutate(model = paste(car, model)) %>% # объединяем производителя и модель для подписей
  mutate_if(is_character, fct_infreq)

glimpse(car_ad1)
  
```

```{r}
summary(car_ad1)
```

```{r}
ggplot(car_ad, aes(x = engV)) +
  geom_histogram() +
  xlim(c(0, 10))
```

```{r}
car_ad1 %>%
  filter(engV > 10) %>%
  arrange(engV)
```

```{r}
car_ad1 <-  car_ad1 %>%
  filter(price > 0) %>%
  mutate(inv_mileage = 1000 / (mileage + 1),
         engV = if_else(engV > 8, as.numeric(NA), engV))
```


```{r}
car_task <- makeRegrTask(id = "Regression", 
                         data = car_ad1, target = "price",
                         fixup.data = "warn") %>%
  #dropFeatures("model") %>%
  mergeSmallFactorLevels(min.perc = 0.015, new.level = ".other")

print(car_task)
```

error
https://github.com/mlr-org/mlr/issues/811

```{r}
car_rdesc <- makeResampleDesc(method = "Holdout", 
                              split = 0.8, 
                              stratify.cols = "car")
car_rdesc
# hout
# cv5, cv10
```

```{r}
reg_learners <- listLearners(obj = "regr")
```


```{r}
lrn_lm <- makeLearner("regr.lm", id = "lm")
lrn_lm


```
```{r}
lrn_ctree <- makeLearner("regr.ctree", id = "ctree")
lrn_ctree
```



```{r}
lrn_rpart <- makeLearner("regr.rpart", id = "rpart")

lrn_rpart
```

```{r}
lrn_glmnet <- makeLearner("regr.glmnet", id = "glmnet")

lrn_glmnet
```

```{r, eval=FALSE}
lrn_lasso <- makeLearner("regr.penalized.lasso", id = "lasso")

lrn_lasso
```



```{r}
#m_lm <- train(car_lm, car_task) # not run
```


```{r}
car_imputation <- car_task %>%
  impute(classes = list(numeric = imputeMean(), factor = imputeConstant("Unknown"))) 

```

```{r}
car_imputed <- car_imputation$task
```

```{r}
m_lm <- train(lrn_lm, car_imputed)

```


```{r}
m_lm %>% 
  getLearnerModel() %>%
  summary()
```


```{r}
set.seed(123)
car_n <- getTaskSize(car_task)
car_train <- sample(car_n, size = car_n * 0.7)
car_test <- base::setdiff(1:car_n, car_train)

```


```{r}
car_mod_lm <- train(lrn_lm, task = car_imputed, subset = car_train)

car_pred_lm <- predict(car_mod_lm, task = car_imputed, subset = car_test)

car_pred_lm
```



```{r}
reg_ms <- list(rmse, mape, mae)

performance(car_pred_lm,
            measures = reg_ms)
```


```{r}
set.seed(123)
resample(learner = lrn_lm, resampling = car_rdesc, task = car_imputed,
         measures = reg_ms)

```

```{r}
car_rdesc_cv <- makeResampleDesc(method = "CV",
                                 iters = 10,
                                 stratify.cols = "car")
```

```{r}
set.seed(123)

resample(learner = lrn_lm, resampling = car_rdesc_cv, task = car_imputed,
         measures = reg_ms)

```





```{r}
set.seed(123)

resample(learner = car_glmnet, resampling = car_rdesc_cv, task = car_imputed,
         measures = list(rmse, mape))
```

```{r}
d <- getTaskData(car_imputed)
head(d)

summary(d$car)
```

```{r}
summary(d$body)
```

```{r}
summary(d$engType)
```

```{r}
summary(d$registration)
```

```{r}
summary(d$drive)
```

```{r}
summary(d$model)
```

```{r}
new_car <- tibble(car = factor("Ford", levels = levels(d$car)),
                  body = factor("crossover", levels = levels(d$body)),
                  mileage = 10, engV = 2.7,
                  inv_mileage = 1000 / (1 + 10),
                  engType = factor("Gas", levels = levels(d$engType)),
                  registration = factor("yes", levels = levels(d$registration)),
                  year = 2012,
                  drive = factor("full", levels = levels(d$drive)),
                  model = factor(".other", levels = levels(d$model))
                  )

new_predict <- predict(car_mod_lm, newdata = new_car)
new_predict 
```
```{r}
new_car2 <- tibble(car = "Ford", body = "crossover",
              mileage = 10L, engV = 2.7,
              inv_mileage = 1000 / (10 + 1),
              engType = "Gas", registration = "yes",
              year = 2012L, drive = "full", model = ".other"
              )

new_car2_imp <- new_car2 %>%
  mutate_if(is.character, as.factor) %>%
  reimpute(desc = car_imputation$desc)

predict(car_mod_lm, newdata = new_car2)
predict(car_mod_lm, newdata = new_car2_imp)
predict(car_mod_lm, newdata = new_car)

str(new_car2)
str(new_car)
str(new_car2_imp)
```
```{r}
str(new_car2_imp)
```

```{r}
plotLearnerPrediction(lrn_lm, task = car_imputed,
                      features = "inv_mileage")
```

```{r}
plotLearnerPrediction(lrn_lm, task = car_imputed,
                      features = c("inv_mileage", "engV"))
```

```{r}
set.seed(123)

resample(learner = car_rpart, resampling = car_rdesc_cv, task = car_imputed,
         measures = list(rmse, mape))
```



```{r}
car_learners <- makeLearners(c("lm", "glmnet", "rpart", "ctree"),
                            ids = c("lm", "glmnet", "rpart", "ctree"),
                            type = "regr")
```

```{r, eval=FALSE}
car_models <- car_learners %>%
  lapply(train, task = car_imputed)
```

```{r}
car_learners %>%
  lapply(plotLearnerPrediction, 
         task = car_imputed,
         features = c("inv_mileage", "engV"))

```

```{r, eval=FALSE}
set.seed(123, "L'Ecuyer")
car_learners %>%
  lapply(resample, task = car_imputed,
         resampling = car_rdesc_cv,
         measures = reg_ms)

```

```{r, eval=F}
library(parallelMap)
parallelStartSocket(4, level = "mlr.resample")
car_learners %>%
  lapply(resample, task = car_imputed,
         resampling = car_rdesc_cv,
         measures = reg_ms)
parallelStop()

```

```{r}
set.seed(123, "L'Ecuyer")
library(parallelMap)
parallelStartSocket(4, level = "mlr.benchmark")
car_bench <- car_learners %>%
  benchmark(tasks = car_imputed,
         resampling = car_rdesc_cv,
         measures = append(list(timetrain), reg_ms))
parallelStop()

car_bench %>%
  getBMRAggrPerformances(as.df = TRUE) %>%
  arrange(mape.test.mean)

```

```{r}
car_bench %>%
  getBMRAggrPerformances(as.df = TRUE) %>%
  arrange(mape.test.mean) %>%
  ggplot(aes(x = fct_inorder(learner.id),
             y = mape.test.mean)) +
  geom_bar(stat = "identity") +
  geom_line(aes(y = timetrain.test.mean, group = 1),
            color = "red") +
  scale_y_continuous(labels = scales::percent,
                     sec.axis = sec_axis(~ . * 1000,
                                         name = "Время обучения, мс"))
```
https://mlr-org.github.io/mlr-tutorial/release/html/benchmark_experiments/index.html

# Задача классификации

```{r}
turnover <- readRDS("turnover.RDS")
head(turnover)

```


```{r, fig.width=10}
turnover %>%
  ggplot(aes(x = satisfaction_level, y = average_monthly_hours,
             color = left,
             size = last_evaluation >= 8)) +
  geom_jitter(alpha = 0.1)
```

```{r}
summary(turnover)
```

```{r}
turnover_task <- makeClassifTask(id = "Classification",
                                 data = turnover,
                                 target = "left",
                                 positive = "left")
turnover_task
```

```{r}
clas_learners <- listLearners("classif")

```


```{r}
turnover_learners <- 
  makeLearners(cls = c("logreg", "rpart", "ctree", "C50"),
               type = "classif", predict.type = "prob")
```

```{r}
turnover_rdesc <- makeResampleDesc(method = "CV", stratify = TRUE,
                                   iter = 10)
```


```{r}
listMeasures()
```


```{r}
set.seed(123, "L'Ecuyer")
clas_ms = list(auc, kappa, mmce, tpr, tnr, fpr)

library(parallelMap)

parallelStartSocket(4, level = "mlr.benchmark")
turnover_bench <- turnover_learners %>%
  benchmark(tasks = turnover_task,
         resampling = turnover_rdesc,
         measures = append(list(timetrain), clas_ms))
parallelStop()

```

```{r}
turnover_bench %>%
  getBMRAggrPerformances(as.df = TRUE) %>%
  arrange(desc(auc.test.mean)) %>%
  select(-task.id) %>%
  mutate_if(is.numeric, round, digits = 3)
```

```{r}
turnover_bench %>%
  getBMRAggrPerformances(as.df = TRUE) %>%
  arrange(desc(auc.test.mean)) %>%
  ggplot(aes(x = fct_inorder(learner.id))) +
           geom_bar(aes(y = auc.test.mean), stat = "identity") +
           geom_line(aes(y = timetrain.test.mean * 5, group = 1),
                     colour = "red") +
  scale_y_continuous(sec.axis = sec_axis(~ . * 1000 / 5, 
                                         name = "Время обучения, мс"),
                     labels = scales::percent)

```


```{r}
turnover_learners %>%
  lapply(plotLearnerPrediction, task = turnover_task,
         features = c("satisfaction_level",
                      "average_monthly_hours"))
```


```{r}

turnover_rpart <- turnover_learners$classif.rpart
turnover_rpart
```
```{r}
set.seed(123)
turnover_n <- getTaskSize(turnover_task)
turnover_train <- sample(turnover_n, size = 0.7 * turnover_n)
turnover_test <- setdiff(1:turnover_n, turnover_train)
```

```{r}
turnover_m_rpart <- train(learner = turnover_rpart,
                          task = turnover_task,
                          subset = turnover_train)
turnover_pr_rpart <- predict(turnover_m_rpart,
                             task = turnover_task,
                             subset = turnover_test)
```

```{r}
performance(turnover_pr_rpart,
            measures = clas_ms) %>%
  round(4)
```

```{r}

turnover_roc <- generateThreshVsPerfData(turnover_pr_rpart, 
                         measures = list(fpr, tpr, tnr, kappa))
plotThreshVsPerf(turnover_roc)
```

```{r}
plotROCCurves(turnover_roc)
```


# Задача кластеризации

```{r}
data(iris)
head(iris)
```

```{r}
glimpse(iris)
```

```{r}
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
  geom_point()
```

```{r}
calculateConfusionMatrix(turnover_pr_rpart)
```

```{r}
plotROCCurves(turnover_roc)
  
```

```{r}
turnover_models <- 
  turnover_learners %>%
  lapply(train, task = turnover_task, subset = turnover_train)

turnover_predictions <- 
  turnover_models %>%
  lapply(predict, task = turnover_task, subset = turnover_test)
```


```{r}
turnover_roc2 <- 
  generateThreshVsPerfData(turnover_predictions,
                           measures = list(fpr, tpr))

plotROCCurves(turnover_roc2)
```
```{r}
turnover_roc2$data %>%
  ggplot(aes(fpr, tpr, colour = learner)) +
  geom_path() +
  geom_abline(slope = 1, linetype = "dashed")
```

```{r}
head(turnover_roc2$data)
```

https://mlr-org.github.io/mlr-tutorial/release/html/roc_analysis/index.html

```{r}
getParamSet(turnover_rpart)
```

http://mlr-org.github.io/Exploring-and-Understanding-Hyperparameter-Tuning/

```{r}
ps_rpart = makeParamSet(
  makeNumericParam("cp", lower = -4, upper = -2,
                   trafo = function(x) 10^x),
  makeIntegerParam("minsplit", lower = 4, upper = 20))

control_rpart = makeTuneControlGenSA(maxit = 100)
cv_rdesc = makeResampleDesc("CV", iters = 5)


```

```{r}
parallelStartSocket(4)#, level = "mlr.tuneParams")

res_rpart = tuneParams(turnover_rpart,
                       measures = list(auc),
                       task = turnover_task,
                       control = control_rpart,
                       resampling = cv_rdesc,
                       par.set = ps_rpart,
                       show.info = FALSE)

parallelStop()
print(res_rpart)
```


```{r}
generateHyperParsEffectData(res_rpart) %>%
  plotHyperParsEffect(x = "cp", y = "auc.test.mean",
                      plot.type = "line",
                      #z = "minsplit",
                      facet = "minsplit")
```

```{r}
generateHyperParsEffectData(res_rpart) %>%
  plotHyperParsEffect(x = "cp", y = "minsplit",
                      z = "auc.test.mean",
                      plot.type = "heatmap",
                      interpolate = "regr.earth",
                      show.experiments = TRUE)
```
http://mlr-org.github.io/Exploring-and-Understanding-Hyperparameter-Tuning/

```{r}
turnover_rpart_opt <-  makeLearner("classif.rpart", 
                                   predict.type = "prob",
                                   par.vals = 
                                     list(cp = 0.000109, 
                                          minsplit = 20))

m_rpart_opt <- train(turnover_rpart_opt,
                     task = turnover_task,
                     subset = turnover_train)

pr_rpart_opt <- predict(m_rpart_opt, task = turnover_task,
                        subset = turnover_test)

turnover_rpart_compare <- 
  generateThreshVsPerfData(
    list("rpart" = turnover_predictions$classif.rpart,
         "rpart opt" = pr_rpart_opt),
    measures = list(fpr, tpr))

```

```{r}
turnover_rpart_compare$data %>%
  ggplot(aes(x = fpr, y = tpr, color = learner)) +
  geom_path() + 
  geom_abline(slope = 1, linetype = "dashed",
              color = "darkgray")
```

```{r}
library(rpart.plot)
getLearnerModel(model = m_rpart_opt) %>%
  rpart.plot()
```

```{r}

list(rpart = turnover_rpart, 
     rpart_opt = turnover_rpart_opt) %>%
  lapply(plotLearnerPrediction,
         task = turnover_task,
         features = c("satisfaction_level",
                      "average_monthly_hours"))
```
https://github.com/mlr-org/mlr/issues/811
